Strings:
slicing: s[0:n], s[:n], s[0:-1]
reverse: s[::-1]
remove/strip spaces: s.strip(), s.lstrip(), s.rstrip()
search: s.find("txt",0,n), s.count("txt",0,n)
replace: s.replace("txt","replace")

list=[8,9,'']
repetetion : lst*4
lst.append(), lst.remove(), lst.clear(),lst.max(),lst.insert(),lst.sort(reverse=true), del(lst[1])
tup=()
tpl1=tuple(lst) -- tuple of list
set={1,2,3}
set.remove(), set.update(1,5)  --frozenset() has no update/delete/
dict={'a':1}    dict.keys(), dict.items(), dict.values()

for i in range(5):

print(a,b,sep='==='),print("name is",name),print("name is %s"%(name))
print("name is {},marks are {}".format(name,number))

https://cmdlinetips.com/2018/01/5-examples-using-dict-comprehension/
http://cmdlinetips.com/2017/05/five-examples-of-using-list-comprehensions-in-python/

# converting string to json 
final_dictionary = json.loads(ini_string)
final_dictionary = eval(ini_string)
----- API testing using Python
python -m pip install --upgrade pip
pip install requests
__name__ is a global variables, set as __main__ when invoking a program directly

1D array [0]*N
2D array [[0]*rows]*col
d = dict() like Hash or Map in Java with key/pair

list is sequential and dict is key based.
list=[8,9,'']
dict ={}, len(), 
tup=[(),()]
tuple=()
Tuples are unchangeable.
tup to dict -- for a,b in tup tup.setDefault(a,[]).append(b)    or dict(tup)

tup=zip(l1,l2) -- then create dict
N = (list(map(int, input().split()))[1:] for _ in range(K))
results = map(lambda x: sum(i**2 for i in x)%M, product(*N))
to make script executable  #!/usr/local/bin/python
random.random()
eval("l."+cmd)  (list.insert(0,0))

#logs
with open(logfile) as f:
        log = f.read()
    print(log)
	
IP regex \b(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\
import read
my_iplist = re.findall(myregex,log)
dict={'remote_host': split_line[0],
            'apache_status': split_line[8],
            'data_transfer': split_line[9],
    }
lines = tuple(open("production.log", 'r'))
line[line.find("ActiveRecord")+14: line.find("ActiveRecord")+19]

df = pd.read_csv("contoh.csv")
g = df.groupby('ID')
g_1 = pd.concat([g.head(1),g.tail(1)]).drop_duplicates().sort_values('ID').reset_index(drop=True)
g_1.to_csv('result.csv')

df.to_csv("test.csv", columns=header,index=True, header=header1)
# zip resulting lists to tuples
zipped = zip(*lst)


    with open('TestData/out.json') as f:
        data = str(json.load(f))
	with open('TestData/out.json',"w+") as f:
        f.write(json.dumps(response.content))
		
#combine multiple rows
df['combined']=df.apply(lambda x:'%s_%s_%s' % (x['bar'],x['foo'],x['new']),axis=1)


gkk = daf.groupby(['Team', 'Position'])
gkk.first()

lst = [['tom', 'reacher', 25], ['krish', 'pete', 30], 
       ['nick', 'wilson', 26], ['juli', 'williams', 22]] 
    
df = pd.DataFrame(lst, columns =['FName', 'LName', 'Age'], dtype = float)
lst = [['tom', 25], ['krish', 30], 
       ['nick', 26], ['juli', 22]] 
    
df = pd.DataFrame(lst, columns =['Name', 'Age'])
lst = ['Geeks', 'For', 'Geeks', 'is', 'portal', 'for', 'Geeks'] 
  
# list of int 
lst2 = [11, 22, 33, 44, 55, 66, 77] 
  
# Calling DataFrame constructor after zipping 
# both lists, with columns specified 
#empty df
df = pd.DataFrame(columns=['A','B','C','D','E','F','G'])
df = pd.DataFrame(list(zip(lst, lst2)), 
               columns =['Name', 'val'])
df = pd.DataFrame(lst, index =['a', 'b', 'c', 'd', 'e', 'f', 'g'], 
                                              columns =['Names'])
https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/
with open('swapspot.csv', 'wb') as myfile:
    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
    wr.writerow(pd.DataFrame(datas[1]))
	
curl -X GET 'http://jsonplaceholder.typicode.com/todos'
response = requests.get('http://jsonplaceholder.typicode.com/todos')
PARAMS = {'address':location}
r = requests.get(url = URL, params = PARAMS)
https://medium.com/@peter.jp.xie/rest-api-testing-using-python-751022c364b8

response = requests.put(url, data=json.dumps(data), headers=headers)
resp = requests.post(url, data = json.dumps(payload,indent=4))
 if method in ['GET', 'POST', 'PUT', 'PATCH', 'DELETE']:
            if method == 'GET': httpresp = requests.get(url, headers=headers, verify=False)
            elif method == 'POST': httpresp = requests.post(url, data, headers=headers, verify=False)
            elif method == 'PUT': httpresp = requests.put(url, data, headers=headers, verify=False)
            elif method == 'PATCH': httpresp = requests.patch(url, data, headers=headers, verify=False)
            elif method == 'DELETE': httpresp = requests.delete(url, headers=headers, verify=False)
{
    "args": {},
    "data": "{\n    \"key1\": 1,\n    \"key2\": \"value2\"\n}",
    "files": {},
    "form": {},
    "headers": {
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate",
        "Content-Length": "39",
        "Content-Type": "application/json",
        "Host": "httpbin.org",
        "User-Agent": "Python Requests"
    },
    "json": {
        "key1": 1,
        "key2": "value2"
    },
    "origin": "103.115.210.48, 103.115.210.48",
    "url": "https://httpbin.org/post"
}




Cocurrent calls:

from concurrent.futures import ThreadPoolExecutor as PoolExecutor
from concurrent.futures import ProcessPoolExecutor as PoolExecutor
import http.client
import socket
import requests

def get_it(url):
    try:
        # always set a timeout when you connect to an external server
        #connection = http.client.HTTPSConnection(url, timeout=2)

        #connection.request("GET", "/")

        response = requests.get(url)
        print ("hit url")
        print (response)
        return response.text
    except socket.timeout:
        # in a real world scenario you would probably do stuff if the
        # socket goes into timeout
        pass
# create a thread pool of 4 threads
urls = [
    "http://privet.mars.c.ia55.net/metaDataService/getHeaderMappings/dealId/3"
] * 20
with PoolExecutor(max_workers=4) as executor:

    # distribute the 1000 URLs among 4 threads in the pool
    # _ is the body of each page that I'm ignoring right now
    for _ in executor.map(get_it, urls):
        pass
		
		session = requests.Session()
        session.auth = HTTPKerberosAuth(mutual_authentication=OPTIONAL, principal=principal)
        #request = requests.Request('GET',url)
        request = requests.Request('GET', url)
        prepared_request = session.prepare_request(request)
        response = session.send(prepared_request)
Virtual env
$python2 -m pip install virtualenv
$virtualnv venv (in proj folder)
$source venv/bin/activate
---- Multiple sheets
dff = pd.ExcelFile(filepath)
		# If sheets are in the same file. 
		xlsx = pd.ExcelFile(filepath)
		df1 = pd.read_excel(xlsx, 'Sheet')
		df2 = pd.read_excel(xlsx, 'Equity Net MV')
		# If sheets are in different files. 
		"""
		df1 = pd.read_excel('path_to_file.xls', sheet_name = "Sheet")
		df2 = pd.read_excel('path_to_file.xls', sheet_name = "Equity Net MV")
		"""
		# Use pandas.concat(), pandas.merge, or DataFrame.join() to join the DataFrames
		df = df1.join(df2)
 
		# Save the DataFrame to a CSV. 
		#df.to_csv(dest_filename)
		PC_sheets = []
		for sheet in xlsx.sheet_names:
			PC_sheets.append(xlsx.parse(sheet))
			dest_filename = self.input_folder_path +'/' + os.path.splitext(os.path.basename(filepath))[0][:-2] +sheet+ os.path.splitext(os.path.basename(filepath))[0][-2:]+  '.csv'
			print (dest_filename)
			print sheet
			PC = pd.read_excel(xlsx, sheet)
			PC.index.name='KEY'
			PC.to_csv(dest_filename, encoding='utf-8', index=True)
			PC.index.name='KEY'
		cols=PC.columns

	def send_results_mail(self):
		"""
		take the result list of maps and send an email and even store in txt file to use for rapid
		"""
		result_str = "<table style ='border : 1px solid black;'><tr><th width='350' style='border: 1px solid black ;'>TestCase Name</th><th width='350' style='border: 1px solid black ;'>CsvConversionStatus</th><th width='350' style='border: 1px solid black ;'>Test Result</th><th width='350' style='border: 1px solid black ;'>Output FilePath</th></tr>"
		temp_row = ""

		for test in self.result_list:
			if test['Status'].lower() == "fail":
				temp_row = temp_row + "<tr bgcolor='#F14741'><td width='350' style='border: 1px solid black ;'>" + str(
					test["TestCaseName"]) + "</td><td width='350' style='border: 1px solid black ;>'>" + str(
					test['BothCsvPresent']) + "</td><td width='350' style='border: 1px solid black ;>'>" + str(
					test['Status']) + "</td><td width='350' style='border: 1px solid black ;>'>" + str(
					test['Diff-Path']) + "</td></tr>"
			else:
				temp_row = temp_row + "<tr bgcolor='#7EF141'><td width='350' style='border: 1px solid black ;'>" + str(
					test["TestCaseName"]) + "</td><td width='350' style='border: 1px solid black ;>'>" + str(
					test['BothCsvPresent']) + "</td><td width='350' style='border: 1px solid black ;>'>" + str(
					test['Status']) + "</td><td width='350' style='border: 1px solid black ;>'>" + str(
					test['Diff-Path']) + "</td></tr>"
		result_str = result_str + temp_row + "</table>"

		# SEND RESULT AS AN EMAIL
		msg = MIMEText(result_str, 'html')
		msg['To'] = email.utils.formataddr(('Recipient', 'kumarh@arcesium.com'))
		msg['From'] = email.utils.formataddr(('Automation', 'kumarh@arcesium.com'))
		msg['Subject'] = self.Subject
		sender = 'kumarh@arcesium.com'
		receivers = 'kumarh@arcesium.com'
		smtpObj = smtplib.SMTP('squire.nyc.deshaw.com')
		smtpObj.sendmail(sender, receivers, msg.as_string())
		
		
		
def main():
	print("Triggering  Rec Diff Process")
	# Reading the input parameters and validating them
	parser = argparse.ArgumentParser()
	parser.add_argument('--InputFolderPath', help='Folder Path where the Rec should be triggered')
	parser.add_argument('--Mode', default='Standalone', help='It runs in 2 modes : Standalone,Integration')
	parser.add_argument('--InputFile1', default='N/A',
						help='When in Integration Mode, specify filenames to be compared')
	parser.add_argument('--InputFile2', default='N/A',
						help='When in Integration Mode, specify filenames to be compared')
	parser.add_argument('--File1Tag', default='PROD', help='When in Integration Mode, specify filenames to be compared')
	parser.add_argument('--File2Tag', default='UAT', help='When in Integration Mode, specify filenames to be compared')
	parser.add_argument('--InputParamMap', default='./Input-Param-Map.txt', help='path from where the input param file is to be picked')
	parser.add_argument('--Subject', help='subject line for script and sync test results')
	parser.add_argument('--TestCaseMap', default='/Zephyre-Map.txt', help='path from where the input param file is to be picked')
	parser.add_argument('--Release', default='March2019', help='Fix Version')
	parser.add_argument('--Cycle', default='Regression Automation', help='QA Cycle')
	parser.set_defaults(feature=True)
	args = parser.parse_args()
	Trigger_Diff(folder_path=args.InputFolderPath, mode=args.Mode, input_file_name_1=args.InputFile1,
				 input_file_name_2=args.InputFile2, file1_tag=args.File1Tag, file2_tag=args.File2Tag,
				 InputParameters=args.InputParamMap, Subject=args.Subject, TestCaseMap=args.TestCaseMap, Release=args.Release, Cycle=args.Cycle)
https://kb.objectrocket.com/mongo-db/how-to-query-mongodb-documents-in-python-269#multiple+conditions+queries+and+pymongo+requests